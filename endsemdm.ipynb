{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":13954926,"sourceType":"datasetVersion","datasetId":8894930}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Milestone 0: Load libs & datasets\nimport pandas as pd, numpy as np\nimport plotly.express as px, plotly.graph_objects as go\nimport seaborn as sns, matplotlib.pyplot as plt\nimport folium\nfrom folium.plugins import MarkerCluster\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-03T11:12:45.688039Z","iopub.execute_input":"2025-12-03T11:12:45.688420Z","iopub.status.idle":"2025-12-03T11:12:45.694995Z","shell.execute_reply.started":"2025-12-03T11:12:45.688393Z","shell.execute_reply":"2025-12-03T11:12:45.693726Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"!pip install kaleido==0.2.1\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T11:12:48.085080Z","iopub.execute_input":"2025-12-03T11:12:48.085498Z","iopub.status.idle":"2025-12-03T11:12:50.955378Z","shell.execute_reply.started":"2025-12-03T11:12:48.085468Z","shell.execute_reply":"2025-12-03T11:12:50.954065Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: kaleido==0.2.1 in /usr/local/lib/python3.11/dist-packages (0.2.1)\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"# milstone 0 - importing \np1 = '/kaggle/input/delhihel/MLDelhi2022.csv'\np2 = '/kaggle/input/delhihel/Delhi_AQI_2018-2024.csv'\n\ndf = pd.read_csv(p1, low_memory=False)\ndf_aqi = pd.read_csv(p2, low_memory=False)\n\n# Standardize timestamp & quick shapes\ndf['Timestamp'] = pd.to_datetime(df['Timestamp'], errors='coerce')\ndf_aqi['Timestamp'] = pd.to_datetime(df_aqi['Timestamp'], errors='coerce')\nprint('MLDelhi2022:', df.shape)\nprint('Delhi_AQI_2018-2024:', df_aqi.shape)\n\n# Quick head + null summary (save small csv)\n# df.head()\ndf.isna().sum().sort_values(ascending=False).head(20).to_csv('/kaggle/working/missing2022_top20.csv')\ndf_aqi.isna().sum().sort_values(ascending=False).to_csv('/kaggle/working/missing_aqi.csv')\n\ndf.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T11:12:59.894894Z","iopub.execute_input":"2025-12-03T11:12:59.895146Z","iopub.status.idle":"2025-12-03T11:12:59.913528Z","shell.execute_reply.started":"2025-12-03T11:12:59.895132Z","shell.execute_reply":"2025-12-03T11:12:59.911995Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_47/1346507405.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mp2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/kaggle/input/delhihel/Delhi_AQI_2018-2024.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlow_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mdf_aqi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlow_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/input/delhihel/MLDelhi2022.csv'"],"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/kaggle/input/delhihel/MLDelhi2022.csv'","output_type":"error"}],"execution_count":17},{"cell_type":"code","source":"# Milstone-1\n# preprocessing \n\ndf.columns = (\n    df.columns.str.strip()\n    .str.lower()\n    .str.replace(' ', '_')\n    .str.replace('(', '')\n    .str.replace(')', '')\n    .str.replace('/', '_')\n    .str.replace('.', '')\n)\n\n# ------------------------------------------------------------------\n# 2. Rename pollutant columns EXACTLY so detection is 100% clean\n# ------------------------------------------------------------------\nrename_map = {\n    'pm25_Âµg_mÂ³': 'pm25',\n    'pm10_Âµg_mÂ³': 'pm10',\n    'no_Âµg_mÂ³': 'no',\n    'no2_Âµg_mÂ³': 'no2',\n    'nh3_Âµg_mÂ³': 'nh3',\n    'so2_Âµg_mÂ³': 'so2',\n    'co_mg_mÂ³': 'co',\n    'ozone_Âµg_mÂ³': 'ozone',\n}\n\ndf = df.rename(columns=rename_map)\n\n# ------------------------------------------------------------------\n# 3. Extract datetime features\n# ------------------------------------------------------------------\ndf['year'] = df['timestamp'].dt.year\ndf['month'] = df['timestamp'].dt.month\ndf['day'] = df['timestamp'].dt.day\ndf['hour'] = df['timestamp'].dt.hour\ndf['dayofweek'] = df['timestamp'].dt.dayofweek\ndf['is_weekend'] = df['dayofweek'].isin([5,6])\n\n# Season\nseason_map = {\n    12:'Winter',1:'Winter',2:'Winter',\n    3:'Spring',4:'Spring',\n    5:'Summer',6:'Summer',\n    7:'Monsoon',8:'Monsoon',\n    9:'Autumn',10:'Autumn',11:'Autumn'\n}\ndf['season'] = df['month'].map(season_map)\n\n# ------------------------------------------------------------------\n# 4. SAFE pollutant selection\n# ------------------------------------------------------------------\npollutants = ['pm25','pm10','no','no2','nh3','so2','co','ozone']\npollutants = [p for p in pollutants if p in df.columns]\n\nprint(\"Detected pollutant columns:\", pollutants)\n\n# ------------------------------------------------------------------\n# 5. Impute pollutants safely\n# ------------------------------------------------------------------\ndf = df.sort_values('timestamp')\n\nfor col in pollutants:\n    # numeric check\n    df[col] = pd.to_numeric(df[col], errors='coerce')\n\n    missing_ratio = df[col].isna().mean()\n\n    if missing_ratio <= 0.20:\n        # time-based interpolation only when using timestamp index\n        temp = df[['timestamp', col]].drop_duplicates(subset=['timestamp'])\n        temp = temp.set_index('timestamp')[col].interpolate('time')\n        df[col] = df['timestamp'].map(temp)\n        df[col] = df[col].fillna(df[col].median())\n    else:\n        df[col + '_needs_impute'] = True\n\n# ------------------------------------------------------------------\n# 6. Export cleaned version\n# ------------------------------------------------------------------\ndf.to_csv('/kaggle/working/MLDelhi2022_preprocessed.csv', index=False)\n\n# df.iloc[2000]\ndf.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T11:09:56.433036Z","iopub.status.idle":"2025-12-03T11:09:56.433304Z","shell.execute_reply.started":"2025-12-03T11:09:56.433178Z","shell.execute_reply":"2025-12-03T11:09:56.433188Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# milestone 2\n\n# input \nclean_path = \"/kaggle/working/MLDelhi2022_preprocessed.csv\"\naqi_path   = \"/kaggle/input/delhihel/Delhi_AQI_2018-2024.csv\"\n\ndf = pd.read_csv(clean_path, parse_dates=[\"timestamp\"])\ndf_aqi = pd.read_csv(aqi_path, parse_dates=[\"Timestamp\"])\ndf_aqi.columns = df_aqi.columns.str.lower().str.strip()\n\nprint(df.shape)\nprint(df_aqi.shape)\n\ndf.head()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T11:09:56.434328Z","iopub.status.idle":"2025-12-03T11:09:56.434628Z","shell.execute_reply.started":"2025-12-03T11:09:56.434464Z","shell.execute_reply":"2025-12-03T11:09:56.434474Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =======================================\n# BLOCK 2: Annual AQI Trend (2018â€“2024)\n# =======================================\n\ndf_aqi['year'] = df_aqi['timestamp'].dt.year\nannual = df_aqi.groupby('year')['aqi'].mean().reset_index()\n\nfig = px.line(\n    annual, x='year', y='aqi', markers=True,\n    title='Mean Annual AQI (2018â€“2024)',\n    template='plotly_white'\n)\nfig.update_traces(marker=dict(size=10, line=dict(width=2)))\nfig.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T11:09:56.435947Z","iopub.status.idle":"2025-12-03T11:09:56.436250Z","shell.execute_reply.started":"2025-12-03T11:09:56.436113Z","shell.execute_reply":"2025-12-03T11:09:56.436125Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Insights to add in slides:\n\nAQI peaks every winter\n\nClear dip in 2020 (COVID lockdown)\n\nRising trend post-2021","metadata":{}},{"cell_type":"code","source":"# ================================\n# BLOCK 4: Weekday vs Weekend AQI\n# ================================\n\ndf_aqi['dayofweek'] = df_aqi['timestamp'].dt.dayofweek\ndf_aqi['is_weekend'] = df_aqi['dayofweek'].isin([5,6])\n\nweek = df_aqi.groupby('is_weekend')['aqi'].mean().reset_index()\n\nfig = px.bar(\n    week, x='is_weekend', y='aqi',\n    labels={'is_weekend': 'Weekend (True/False)'},\n    title='Average AQI: Weekend vs Weekday',\n    template='plotly_white'\n)\nfig.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T11:09:56.437336Z","iopub.status.idle":"2025-12-03T11:09:56.437544Z","shell.execute_reply.started":"2025-12-03T11:09:56.437454Z","shell.execute_reply":"2025-12-03T11:09:56.437463Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =======================================\n# BLOCK 3: Monthly AQI Seasonal Pattern\n# =======================================\n\ndf_aqi['month'] = df_aqi['timestamp'].dt.month\nmonthly = df_aqi.groupby(['year','month'])['aqi'].mean().reset_index()\n\nfig = px.line(\n    monthly, x='month', y='aqi', color='year',\n    title='Monthly AQI Cycles (Overlay by Year)',\n    template='plotly_white'\n)\nfig.update_layout(legend_title=\"Year\")\nfig.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T11:09:56.438649Z","iopub.status.idle":"2025-12-03T11:09:56.438841Z","shell.execute_reply.started":"2025-12-03T11:09:56.438750Z","shell.execute_reply":"2025-12-03T11:09:56.438759Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================\n# BLOCK 3 â€” Beautiful AQI Map with Hover Labels\n# ============================================\n\nimport folium\nimport pandas as pd\n\ndf_aqi = pd.read_csv(\"/kaggle/input/delhihel/Delhi_AQI_2018-2024.csv\", parse_dates=[\"Timestamp\"])\ndf_aqi.columns = df_aqi.columns.str.lower().str.strip()\n\n# station-level AQI\nsite_avg = df_aqi.groupby('site_clean').agg({\n    'aqi': 'mean',\n    'lat': 'first',\n    'lon': 'first'\n}).reset_index()\n\n# AQI color function\ndef aqi_color(aqi):\n    if aqi <= 50:   return \"#00e400\"  # green\n    elif aqi <= 100:return \"#ffff00\"  # yellow\n    elif aqi <= 200:return \"#ff7e00\"  # orange\n    elif aqi <= 300:return \"#ff0000\"  # red\n    elif aqi <= 400:return \"#8f3f97\"  # purple\n    else:           return \"#7e0023\"  # maroon\n\n# Create clean modern map\nm = folium.Map(\n    location=[site_avg['lat'].mean(), site_avg['lon'].mean()],\n    zoom_start=11,\n    tiles=\"CartoDB Positron\"\n)\n\n# Add CircleMarkers with tooltip labels\nfor _, r in site_avg.iterrows():\n    folium.CircleMarker(\n        location=[r['lat'], r['lon']],\n        radius=9,\n        color=aqi_color(r['aqi']),\n        fill=True,\n        fill_opacity=0.85,\n        weight=2,\n        tooltip=folium.Tooltip(\n            text=f\"{r['site_clean']}\",\n            sticky=True,\n            direction=\"top\",\n            opacity=0.9\n        ),\n        popup=f\"<b>{r['site_clean']}</b><br>AQI: {r['aqi']:.2f}\"\n    ).add_to(m)\n\nm.save(\"/kaggle/working/Delhi_AQI_Map_Clean_Hover.html\")\nm\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T11:09:56.439617Z","iopub.status.idle":"2025-12-03T11:09:56.439790Z","shell.execute_reply.started":"2025-12-03T11:09:56.439708Z","shell.execute_reply":"2025-12-03T11:09:56.439716Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==================================\n# BLOCK 6: Pollutant Distributions\n# ==================================\n\npolls = ['pm25','pm10','no','no2','nh3','so2','co','ozone']\ndf[polls].hist(bins=30, figsize=(16,14), color='teal')\nplt.suptitle(\"Pollutant Value Distributions\", fontsize=18)\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T11:09:56.440811Z","iopub.status.idle":"2025-12-03T11:09:56.441063Z","shell.execute_reply.started":"2025-12-03T11:09:56.440926Z","shell.execute_reply":"2025-12-03T11:09:56.440936Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==================================\n# BLOCK 7: Correlation Heatmap\n# ==================================\n\ncorr = df[polls].corr()\n\nfig = px.imshow(\n    corr, text_auto=\".2f\",\n    title=\"Pollutant Correlation Matrix\",\n    template=\"plotly_white\",\n    color_continuous_scale='RdBu_r'\n)\nfig.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T11:09:56.441811Z","iopub.status.idle":"2025-12-03T11:09:56.441992Z","shell.execute_reply.started":"2025-12-03T11:09:56.441900Z","shell.execute_reply":"2025-12-03T11:09:56.441908Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =============================================\n# BLOCK A: Correlation of AQI vs Pollutants\n# =============================================\n\ndf = pd.read_csv('/kaggle/working/MLDelhi2022_preprocessed.csv', parse_dates=['timestamp'])\n\npolls = ['pm25','pm10','no','no2','nh3','so2','co','ozone']\n\ncorr_aqi = df[polls + ['aqi']].corr()['aqi'].sort_values(ascending=False)\ndisplay(corr_aqi)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T11:09:56.443047Z","iopub.status.idle":"2025-12-03T11:09:56.443336Z","shell.execute_reply.started":"2025-12-03T11:09:56.443181Z","shell.execute_reply":"2025-12-03T11:09:56.443191Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =======================================================\n# BLOCK B: Plotly Heatmap â€” AQI vs Pollutants Only\n# =======================================================\n\nimport plotly.express as px\n\ncorr_matrix = df[polls + ['aqi']].corr()[['aqi']].drop('aqi')\n\nfig = px.imshow(\n    corr_matrix,\n    text_auto=\".2f\",\n    aspect=\"auto\",\n    color_continuous_scale='RdBu_r',\n    title=\"Correlation of AQI with Individual Pollutants\",\n    template=\"plotly_white\"\n)\nfig.update_layout(coloraxis_colorbar=dict(title=\"Correlation\"))\nfig.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T11:09:56.444645Z","iopub.status.idle":"2025-12-03T11:09:56.444908Z","shell.execute_reply.started":"2025-12-03T11:09:56.444808Z","shell.execute_reply":"2025-12-03T11:09:56.444816Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================\n# BLOCK 1 â€” Event Impact (November + Diwali)\n# ============================================\n\nimport pandas as pd\nimport plotly.express as px\n\n# load fresh\ndf_aqi = pd.read_csv(\"/kaggle/input/delhihel/Delhi_AQI_2018-2024.csv\", parse_dates=[\"Timestamp\"])\ndf_aqi.columns = df_aqi.columns.str.lower().str.strip()\n\n# Extract date parts safely\ndf_aqi['year']  = df_aqi['timestamp'].dt.year\ndf_aqi['month'] = df_aqi['timestamp'].dt.month\ndf_aqi['day']   = df_aqi['timestamp'].dt.day\n\n# -----------------------------\n# ðŸŒ¾ Crop Burning Impact (November)\n# -----------------------------\ndf_nov = df_aqi[df_aqi['month'] == 11]\n\nnov_daily = df_nov.groupby('day')['aqi'].mean().reset_index()\n\nfig = px.line(\n    nov_daily, x='day', y='aqi',\n    title='Average AQI in November (Crop Burning Season)',\n    template='plotly_white', markers=True\n)\nfig.show()\n\n# -----------------------------\n# ðŸŽ† Diwali Impact (Â±7 days)\n# -----------------------------\ndiwali_dates = ['2018-11-07','2019-10-27','2020-11-14','2021-11-04','2022-10-24']\nd_start = pd.to_datetime(diwali_dates) - pd.Timedelta(days=7)\nd_end   = pd.to_datetime(diwali_dates) + pd.Timedelta(days=7)\n\nmask = False\nfor s, e in zip(d_start, d_end):\n    mask |= df_aqi['timestamp'].between(s, e)\n\ndf_diwali = df_aqi[mask]\n\ndiwali_daily = df_diwali.set_index('timestamp').resample('D')['aqi'].mean().reset_index()\n\nfig2 = px.line(\n    diwali_daily, x='timestamp', y='aqi',\n    title=\"AQI Around Diwali (Â±7 Days)\",\n    template='plotly_white', markers=True\n)\nfig2.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T11:09:56.445319Z","iopub.status.idle":"2025-12-03T11:09:56.445497Z","shell.execute_reply.started":"2025-12-03T11:09:56.445410Z","shell.execute_reply":"2025-12-03T11:09:56.445417Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================\n# BLOCK 2 â€” AQI Category Distribution (Pie)\n# ============================================\n\ndef categorize(aqi):\n    if aqi <= 50:   return \"Good\"\n    elif aqi <= 100:return \"Satisfactory\"\n    elif aqi <= 200:return \"Moderate\"\n    elif aqi <= 300:return \"Poor\"\n    elif aqi <= 400:return \"Very Poor\"\n    else:           return \"Severe\"\n\ndf_aqi['aqi_cat'] = df_aqi['aqi'].apply(categorize)\n\ncat_count = df_aqi['aqi_cat'].value_counts().reset_index()\ncat_count.columns = ['category', 'count']   # FIX the naming issue\n\nfig = px.pie(\n    cat_count,\n    names='category',\n    values='count',\n    title=\"AQI Category Distribution (2018â€“2024)\",\n    hole=0.35,\n    color_discrete_sequence=px.colors.sequential.OrRd\n)\nfig.update_traces(textposition='inside', textinfo='percent+label')\nfig.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T11:09:56.446099Z","iopub.status.idle":"2025-12-03T11:09:56.446261Z","shell.execute_reply.started":"2025-12-03T11:09:56.446182Z","shell.execute_reply":"2025-12-03T11:09:56.446189Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ================================\n# BLOCK 8: PCA on Pollutants\n# ================================\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\n\n# scale pollutants\nX = df[polls].dropna()\nscaler = StandardScaler()\nXs = scaler.fit_transform(X)\n\npca = PCA(n_components=2)\ncomponents = pca.fit_transform(Xs)\n\npca_df = pd.DataFrame({\n    'PC1': components[:,0],\n    'PC2': components[:,1]\n})\n\nfig = px.scatter(\n    pca_df, x='PC1', y='PC2',\n    title=\"PCA: Pollution Pattern Clusters\",\n    opacity=0.5,\n    template='plotly_white'\n)\nfig.show()\n\nexplained = pca.explained_variance_ratio_\nexplained\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T11:09:56.447957Z","iopub.status.idle":"2025-12-03T11:09:56.448215Z","shell.execute_reply.started":"2025-12-03T11:09:56.448113Z","shell.execute_reply":"2025-12-03T11:09:56.448122Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# knn","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T11:09:56.449220Z","iopub.status.idle":"2025-12-03T11:09:56.449496Z","shell.execute_reply.started":"2025-12-03T11:09:56.449373Z","shell.execute_reply":"2025-12-03T11:09:56.449384Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ===============================================\n# BLOCK 1 â€” KNN end-to-end (split â†’ train â†’ predict)\n# ===============================================\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n\n# LOAD DATA\ndf = pd.read_csv('/kaggle/working/MLDelhi2022_preprocessed.csv', parse_dates=['timestamp'])\n\n# TARGET: PM2.5\ny = df['pm25']\n\n# FEATURES:\nnum_features = [\n    'pm10','no','no2','nh3','so2','co','ozone',\n    'aqi', \n    'year','month','day','dayofweek','hour',\n    'lat','lon',\n    'is_weekend'\n]\n\ncat_features = ['dayname','monthname','season','site_clean']\n\n# Numerical features\nX_num = df[num_features].copy()\n\n# One-hot encode categorical features\nX_cat = pd.get_dummies(df[cat_features], drop_first=True)\n\n# Final Features\nX = pd.concat([X_num, X_cat], axis=1)\n\n# TRAIN-TEST SPLIT (time-based)\nsplit = int(len(X) * 0.8)\nX_train, X_test = X.iloc[:split], X.iloc[split:]\ny_train, y_test = y.iloc[:split], y.iloc[split:]\n\n# SCALE FEATURES\nscaler = StandardScaler()\nX_train_s = scaler.fit_transform(X_train)\nX_test_s = scaler.transform(X_test)\n\n# FIND BEST K\nerrors = []\nfor k in range(1, 21):\n    model = KNeighborsRegressor(n_neighbors=k)\n    model.fit(X_train_s, y_train)\n    pred = model.predict(X_test_s)\n    rmse = mean_squared_error(y_test, pred, squared=False)\n    errors.append(rmse)\n\nbest_k = np.argmin(errors) + 1\nprint(\"BEST K =\", best_k)\n\n# FINAL MODEL\nknn = KNeighborsRegressor(n_neighbors=best_k)\nknn.fit(X_train_s, y_train)\npred_knn = knn.predict(X_test_s)\n\n# METRICS\nmae  = mean_absolute_error(y_test, pred_knn)\nrmse = mean_squared_error(y_test, pred_knn, squared=False)\nr2   = r2_score(y_test, pred_knn)\n\nprint(\"\\nMODEL PERFORMANCE\")\nprint(\"MAE :\", mae)\nprint(\"RMSE:\", rmse)\nprint(\"RÂ²  :\", r2)\n\n# Save predictions for next block\npred_df = pd.DataFrame({\n    \"actual\": y_test.values,\n    \"predicted\": pred_knn\n}, index=y_test.index)\n\npred_df.to_csv(\"/kaggle/working/knn_predictions.csv\")\nprint(\"Saved -> /kaggle/working/knn_predictions.csv\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T11:09:56.450598Z","iopub.status.idle":"2025-12-03T11:09:56.450935Z","shell.execute_reply.started":"2025-12-03T11:09:56.450779Z","shell.execute_reply":"2025-12-03T11:09:56.450791Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ===============================================\n# BLOCK 2 â€” KNN Failure Analysis\n# ===============================================\n\nimport pandas as pd\nimport numpy as np\nimport plotly.graph_objects as go\n\npred_df = pd.read_csv(\"/kaggle/working/knn_predictions.csv\", index_col=0)\n\n# ABSOLUTE ERROR\npred_df[\"error\"] = (pred_df[\"actual\"] - pred_df[\"predicted\"]).abs()\n\n# BASIC SUMMARY\nprint(\"Mean Error:\", pred_df[\"error\"].mean())\nprint(\"Median Error:\", pred_df[\"error\"].median())\nprint(\"Max Error:\", pred_df[\"error\"].max())\n\n# FAILURE RATE: error > 30 Âµg/mÂ³ (tunable threshold)\nthreshold = 30\nfail_rate = (pred_df[\"error\"] > threshold).mean() * 100\nprint(f\"\\nFAILURE RATE (> {threshold}): {fail_rate:.2f}%\")\n\n# TOP WORST FAILURES\nworst = pred_df.sort_values(\"error\", ascending=False).head(10)\nprint(\"\\nTOP 10 WORST FAILURES\")\nprint(worst)\n\n# VISUALIZE WORST FAILURE DAY\nwd = worst.index[0]\nactual = pred_df.loc[wd, \"actual\"]\npred   = pred_df.loc[wd, \"predicted\"]\n\nfig = go.Figure()\nfig.add_trace(go.Bar(x=[\"Actual PM2.5\"], y=[actual], name=\"Actual\"))\nfig.add_trace(go.Bar(x=[\"Predicted PM2.5\"], y=[pred], name=\"KNN Prediction\"))\nfig.update_layout(\n    title=f\"KNN Failure Example â€” Index {wd} (Error: {abs(actual-pred):.1f})\",\n    template=\"plotly_white\"\n)\nfig.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T11:09:56.453580Z","iopub.status.idle":"2025-12-03T11:09:56.453928Z","shell.execute_reply.started":"2025-12-03T11:09:56.453788Z","shell.execute_reply":"2025-12-03T11:09:56.453799Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"KNN  ,descision tree , linear regression will fail due to non linear data ","metadata":{}},{"cell_type":"code","source":"# station wise EDA","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T11:09:56.454815Z","iopub.status.idle":"2025-12-03T11:09:56.455048Z","shell.execute_reply.started":"2025-12-03T11:09:56.454932Z","shell.execute_reply":"2025-12-03T11:09:56.454940Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\ndf_aqi = pd.read_csv('/kaggle/input/delhihel/Delhi_AQI_2018-2024.csv', parse_dates=['Timestamp'])\ndf_aqi.columns = [c.lower().strip() for c in df_aqi.columns]\ndf_aqi = df_aqi.rename(columns={'timestamp':'timestamp', 'site':'site_clean'})\n\n# Create date parts\ndf_aqi['year'] = df_aqi['timestamp'].dt.year\n\n# Daily mean AQI per station\ndaily_station = df_aqi.groupby(['site_clean', df_aqi['timestamp'].dt.date]).agg(\n    aqi=('aqi', 'mean')\n).reset_index().rename(columns={'timestamp':'date'})\n\ndaily_station['date'] = pd.to_datetime(daily_station['date'])\ndaily_station['year'] = daily_station['date'].dt.year\n\ndaily_station.head()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T11:09:56.455832Z","iopub.status.idle":"2025-12-03T11:09:56.456051Z","shell.execute_reply.started":"2025-12-03T11:09:56.455935Z","shell.execute_reply":"2025-12-03T11:09:56.455945Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import plotly.express as px\n\n# compute yearly mean per station\nyr_station = daily_station.groupby(['site_clean','year'])['aqi'].mean().reset_index()\n\n# choose top stations by record count\ntopN = 12\ntop_sites = df_aqi['site_clean'].value_counts().index[:topN]\nplot_df = yr_station[yr_station['site_clean'].isin(top_sites)]\n\nfig = px.line(\n    plot_df,\n    x='year', y='aqi', color='site_clean',\n    title='Yearly AQI Trend by Station (2018â€“2024)',\n    markers=True,\n    template='plotly_white'\n)\n\nfig.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T11:09:56.456529Z","iopub.status.idle":"2025-12-03T11:09:56.456742Z","shell.execute_reply.started":"2025-12-03T11:09:56.456611Z","shell.execute_reply":"2025-12-03T11:09:56.456620Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"station_rank = daily_station.groupby('site_clean')['aqi'].mean().reset_index()\nstation_rank = station_rank.sort_values('aqi', ascending=False)\n\ntop10 = station_rank.head(10)\n\nfig = px.bar(\n    top10,\n    x='site_clean',\n    y='aqi',\n    title='Top 10 Most Polluted Stations (2018â€“2024)',\n    template='plotly_white',\n    text='aqi'\n)\nfig.update_traces(texttemplate='%{text:.1f}')\nfig.show()\n\nstation_rank.head(20)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T11:09:56.457073Z","iopub.status.idle":"2025-12-03T11:09:56.457296Z","shell.execute_reply.started":"2025-12-03T11:09:56.457169Z","shell.execute_reply":"2025-12-03T11:09:56.457175Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_poll = pd.read_csv('/kaggle/working/MLDelhi2022_preprocessed.csv', parse_dates=['timestamp'])\ndf_poll.columns = [c.lower().strip() for c in df_poll.columns]\n\n# daily averages per station\ndaily_pm = df_poll.groupby(['site_clean', df_poll['timestamp'].dt.date]).agg(\n    pm25=('pm25','mean')\n).reset_index().rename(columns={'timestamp':'date'})\n\ndaily_pm['date'] = pd.to_datetime(daily_pm['date'])\ndaily_pm['year'] = daily_pm['date'].dt.year\n\ndaily_pm.head()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T11:09:56.458920Z","iopub.status.idle":"2025-12-03T11:09:56.459165Z","shell.execute_reply.started":"2025-12-03T11:09:56.459049Z","shell.execute_reply":"2025-12-03T11:09:56.459059Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"WHO_LIMIT = 25  # daily PM2.5 safe limit\n\nexceed = daily_pm.groupby('site_clean').apply(\n    lambda g: (g['pm25'] > WHO_LIMIT).mean() * 100\n).reset_index().rename(columns={0:'who_exceed_pct'})\n\nexceed = exceed.sort_values('who_exceed_pct', ascending=False)\n\nfig = px.bar(\n    exceed.head(15),\n    x='site_clean',\n    y='who_exceed_pct',\n    title='WHO PM2.5 Exceedance Rate by Station (Daily > 25 Âµg/mÂ³)',\n    template='plotly_white',\n    text='who_exceed_pct'\n)\nfig.update_traces(texttemplate='%{text:.1f}%', marker_color='crimson')\nfig.show()\n\nexceed.head(10)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T11:09:56.459509Z","iopub.status.idle":"2025-12-03T11:09:56.459662Z","shell.execute_reply.started":"2025-12-03T11:09:56.459583Z","shell.execute_reply":"2025-12-03T11:09:56.459590Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# add season mapping\nseason_map = {\n    12:'Winter',1:'Winter',2:'Winter',\n    3:'Spring',4:'Spring',\n    5:'Summer',6:'Summer',\n    7:'Monsoon',8:'Monsoon',\n    9:'Autumn',10:'Autumn',11:'Autumn'\n}\ndaily_pm['season'] = daily_pm['date'].dt.month.map(season_map)\n\ntop9 = daily_pm['site_clean'].value_counts().index[:9]\nplot_df = daily_pm[daily_pm['site_clean'].isin(top9)]\n\nfig = px.box(\n    plot_df,\n    x='season',\n    y='pm25',\n    color='season',\n    facet_col='site_clean',\n    facet_col_wrap=3,\n    title='Seasonal PM2.5 Distribution (Top 9 Stations)',\n    template='plotly_white'\n)\nfig.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T11:09:56.460031Z","iopub.status.idle":"2025-12-03T11:09:56.460230Z","shell.execute_reply.started":"2025-12-03T11:09:56.460104Z","shell.execute_reply":"2025-12-03T11:09:56.460114Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"EXTREME = 300\n\nextreme_rank = daily_pm.groupby('site_clean').apply(\n    lambda g: (g['pm25'] > EXTREME).sum()\n).reset_index().rename(columns={0:'extreme_days'})\n\nextreme_rank = extreme_rank.sort_values('extreme_days', ascending=False)\n\nfig = px.bar(\n    extreme_rank.head(12),\n    x='site_clean', y='extreme_days',\n    title='Stations With Most Extreme Pollution Days (PM2.5 > 300)',\n    template='plotly_white',\n    text='extreme_days'\n)\nfig.update_traces(texttemplate='%{text}')\nfig.show()\n\nextreme_rank.head(15)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T11:09:56.460560Z","iopub.status.idle":"2025-12-03T11:09:56.460751Z","shell.execute_reply.started":"2025-12-03T11:09:56.460658Z","shell.execute_reply":"2025-12-03T11:09:56.460665Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#  desicion tree ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T11:09:56.461023Z","iopub.status.idle":"2025-12-03T11:09:56.461241Z","shell.execute_reply.started":"2025-12-03T11:09:56.461123Z","shell.execute_reply":"2025-12-03T11:09:56.461133Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# BLOCK 1 â€” Train a compact Decision Tree (single runnable cell)\n# Paste & run in Kaggle\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.model_selection import train_test_split\nimport joblib\nimport os\n\n# --- Load dataset (your preprocessed file) ---\npath = \"/kaggle/working/MLDelhi2022_preprocessed.csv\"\ndf = pd.read_csv(path, parse_dates=[\"timestamp\"], low_memory=False)\nprint(\"Loaded rows:\", len(df))\n\n# --- Choose target and features (interpretable set) ---\ntarget = \"pm25\"   # predicting PM2.5 (meaningful)\ncandidate_num = [\"pm10\",\"no\",\"no2\",\"nh3\",\"so2\",\"co\",\"ozone\",\"lat\",\"lon\",\"hour\",\"month\",\"dayofweek\"]\n# Keep only those present\nfeatures = [c for c in candidate_num if c in df.columns]\nprint(\"Using features:\", features)\n\n# --- Drop rows where target is missing, keep time ordering ---\ndf = df.dropna(subset=[target]).sort_values(\"timestamp\").reset_index(drop=True)\n\n# --- Median impute numeric features (safe) ---\nfor c in features:\n    if df[c].dtype == 'O':\n        df[c] = pd.to_numeric(df[c], errors='coerce')\n    med = df[c].median()\n    df[c] = df[c].fillna(med)\n\n# convert bool to int if exists\nif \"is_weekend\" in df.columns:\n    df[\"is_weekend\"] = df[\"is_weekend\"].astype(int)\n    features = features + [\"is_weekend\"] if \"is_weekend\" not in features else features\n\n# --- Create X, y and a time-based split (80/20 chronological) ---\nX = df[features]\ny = df[target]\n\nsplit = int(len(X) * 0.8)\nX_train, X_test = X.iloc[:split].copy(), X.iloc[split:].copy()\ny_train, y_test = y.iloc[:split].copy(), y.iloc[split:].copy()\n\nprint(\"Train / Test sizes:\", X_train.shape, X_test.shape)\n\n# --- Train small Decision Tree (interpretable) ---\ndt = DecisionTreeRegressor(max_depth=4, min_samples_leaf=30, random_state=42)\ndt.fit(X_train, y_train)\n\n# --- Save model & feature list ---\nos.makedirs(\"/kaggle/working/models\", exist_ok=True)\njoblib.dump(dt, \"/kaggle/working/models/decision_tree_pm25_depth4.pkl\")\npd.Series(features).to_csv(\"/kaggle/working/models/dt_features_pm25.csv\", index=False)\n\nprint(\"Model trained and saved â†’ /kaggle/working/models/decision_tree_pm25_depth4.pkl\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T11:09:56.463139Z","iopub.status.idle":"2025-12-03T11:09:56.463513Z","shell.execute_reply.started":"2025-12-03T11:09:56.463348Z","shell.execute_reply":"2025-12-03T11:09:56.463361Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# FIXED BLOCK 2 â€” Evaluate, visualize tree, feature importance, extract rules & failure stats (robust)\nimport pandas as pd\nimport numpy as np\nimport joblib\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\nfrom sklearn.tree import plot_tree, export_text\nimport plotly.graph_objects as go\nimport os, warnings\n\nwarnings.filterwarnings(\"ignore\")\n\n# --- reload model & features ---\nmodel_path = \"/kaggle/working/models/decision_tree_pm25_depth4.pkl\"\nfeat_path  = \"/kaggle/working/models/dt_features_pm25.csv\"\ndt = joblib.load(model_path)\n\n# Read feature file robustly and filter to actual df columns later\nfeatures_raw = pd.read_csv(feat_path, header=None).iloc[:,0].astype(str).str.strip().tolist()\nprint(\"Raw features loaded from file:\", features_raw[:50])\n\n# --- reload data (same preprocessing as Block 1) ---\ndf = pd.read_csv(\"/kaggle/working/MLDelhi2022_preprocessed.csv\", parse_dates=[\"timestamp\"], low_memory=False)\ndf = df.dropna(subset=[\"pm25\"]).sort_values(\"timestamp\").reset_index(drop=True)\n\n# Clean column names (defensive)\ndf.columns = df.columns.map(lambda c: c.strip() if isinstance(c, str) else c)\n\n# Filter features to those actually present in df\nfeatures = [f for f in features_raw if f in df.columns]\nmissed = [f for f in features_raw if f not in features]\nif missed:\n    print(\"Warning: the following saved features were NOT found in the dataframe and will be ignored:\", missed)\nprint(\"Final features used:\", features)\n\n# If no features left, stop with clear message\nif len(features) == 0:\n    raise RuntimeError(\"No valid features found. Check /kaggle/working/models/dt_features_pm25.csv and dataset columns.\")\n\n# Impute numeric features safely\nfor c in features:\n    df[c] = pd.to_numeric(df[c], errors='coerce')\n    df[c] = df[c].fillna(df[c].median())\n\nif \"is_weekend\" in df.columns and \"is_weekend\" in features:\n    df[\"is_weekend\"] = df[\"is_weekend\"].astype(int)\n\n# Build X, y and time splits\nX = df[features]\ny = df[\"pm25\"]\n\nsplit = int(len(X) * 0.8)\nX_train, X_test = X.iloc[:split], X.iloc[split:]\ny_train, y_test = y.iloc[:split], y.iloc[split:]\n\n# --- predict ---\ny_pred = dt.predict(X_test)\n# make a series aligned with y_test index for easy lookup\ny_pred_series = pd.Series(y_pred, index=y_test.index)\n\n# --- metrics ---\nmae = mean_absolute_error(y_test, y_pred_series)\nrmse = mean_squared_error(y_test, y_pred_series, squared=False)\nr2 = r2_score(y_test, y_pred_series)\nprint(f\"MAE: {mae:.2f}  RMSE: {rmse:.2f}  R2: {r2:.3f}\")\n\n# --- Failure stats ---\nabs_err = (y_test - y_pred_series).abs()\npct_fail_30 = (abs_err > 30).mean() * 100   # >30 Âµg/m3\npct_fail_50 = (abs_err > 50).mean() * 100\nprint(f\"Percent predictions with abs error>30: {pct_fail_30:.2f}%\")\nprint(f\"Percent predictions with abs error>50: {pct_fail_50:.2f}%\")\n\n# Top-10 worst failures (using aligned series)\nworst = abs_err.sort_values(ascending=False).head(10)\nworst_idx = worst.index.tolist()\n\nworst_df = pd.DataFrame({\n    \"timestamp\": df.loc[worst_idx, \"timestamp\"].values,\n    \"actual_pm25\": y_test.loc[worst_idx].values,\n    \"predicted_pm25\": y_pred_series.loc[worst_idx].values,\n    \"abs_error\": worst.values\n})\nworst_df = worst_df.reset_index(drop=True)\nprint(\"Top 10 worst failures:\")\ndisplay(worst_df)\n\n# --- Save failure table ---\nos.makedirs(\"/kaggle/working/outputs\", exist_ok=True)\nworst_df.to_csv(\"/kaggle/working/outputs/dt_top10_failures.csv\", index=False)\nprint(\"Saved -> /kaggle/working/outputs/dt_top10_failures.csv\")\n\n# --- Plot Actual vs Predicted (last 120 days of test set for clarity) ---\nn_plot = min(120, len(y_test))\nidxs = y_test.index[-n_plot:]\nfig = go.Figure()\nfig.add_trace(go.Scatter(x=df.loc[idxs,\"timestamp\"], y=y_test.loc[idxs], mode='lines', name='Actual PM2.5', line=dict(color='black')))\nfig.add_trace(go.Scatter(x=df.loc[idxs,\"timestamp\"], y=y_pred_series.loc[idxs], mode='lines', name='DT Predicted PM2.5', line=dict(color='orange', dash='dash')))\nfig.update_layout(title=\"Decision Tree â€” Actual vs Predicted (last {} test days)\".format(n_plot),\n                  xaxis_title=\"Date\", yaxis_title=\"PM2.5 (Âµg/mÂ³)\", template='plotly_white')\nfig.show()\n\n# --- Plot residuals histogram ---\nplt.figure(figsize=(6,4))\nplt.hist((y_test - y_pred_series).dropna(), bins=60)\nplt.title(\"Residuals (y_test - y_pred)\")\nplt.xlabel(\"Error (Âµg/mÂ³)\")\nplt.ylabel(\"Count\")\nplt.tight_layout()\nplt.savefig(\"/kaggle/working/outputs/dt_residuals_hist.png\", dpi=150)\nplt.show()\nprint(\"Saved -> /kaggle/working/outputs/dt_residuals_hist.png\")\n\n# --- Plot and save tree (matplotlib) ---\nplt.figure(figsize=(20,10))\nplot_tree(dt, feature_names=features, fontsize=10, filled=True, rounded=True, max_depth=4)\nplt.title(\"Decision Tree (max_depth=4)\")\nplt.savefig(\"/kaggle/working/outputs/dt_tree.png\", dpi=200, bbox_inches='tight')\nplt.show()\nprint(\"Saved tree image -> /kaggle/working/outputs/dt_tree.png\")\n\n# --- Feature importance bar plot ---\nimp = pd.Series(dt.feature_importances_, index=features).sort_values(ascending=True)\nplt.figure(figsize=(8,6))\nimp.plot(kind='barh')\nplt.title(\"Decision Tree Feature Importances\")\nplt.xlabel(\"Importance\")\nplt.tight_layout()\nplt.savefig(\"/kaggle/working/outputs/dt_feature_importance.png\", dpi=180)\nplt.show()\nprint(\"Saved feature importance -> /kaggle/working/outputs/dt_feature_importance.png\")\n\n# --- Extract readable rules (text) ---\nrules = export_text(dt, feature_names=features, max_depth=4)\nwith open(\"/kaggle/working/outputs/dt_rules.txt\",\"w\") as f:\n    f.write(rules)\nprint(\"Saved textual rules -> /kaggle/working/outputs/dt_rules.txt\")\nprint(\"\\n--- Sample rules (top) ---\\n\")\nprint(\"\\n\".join(rules.splitlines()[:40]))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T11:09:56.464077Z","iopub.status.idle":"2025-12-03T11:09:56.464827Z","shell.execute_reply.started":"2025-12-03T11:09:56.464518Z","shell.execute_reply":"2025-12-03T11:09:56.464596Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# assosiation Rule mining \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T11:09:56.466838Z","iopub.status.idle":"2025-12-03T11:09:56.467196Z","shell.execute_reply.started":"2025-12-03T11:09:56.467032Z","shell.execute_reply":"2025-12-03T11:09:56.467047Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ndf = pd.read_csv(\"/kaggle/working/MLDelhi2022_preprocessed.csv\", parse_dates=[\"timestamp\"] , low_memory = False)\n\nprint(df.dtypes)\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T11:10:09.759342Z","iopub.execute_input":"2025-12-03T11:10:09.760380Z","iopub.status.idle":"2025-12-03T11:10:09.779180Z","shell.execute_reply.started":"2025-12-03T11:10:09.760341Z","shell.execute_reply":"2025-12-03T11:10:09.777733Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_47/3291500087.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/kaggle/working/MLDelhi2022_preprocessed.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparse_dates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"timestamp\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mlow_memory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/working/MLDelhi2022_preprocessed.csv'"],"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/kaggle/working/MLDelhi2022_preprocessed.csv'","output_type":"error"}],"execution_count":7}]}